
Archived entries from file /Users/tonyday/projects/hyperq/hyperq.org


* Archived Tasks

** Controller.hs
:PROPERTIES:
:tangle:   haskell/hyperqdev/Controller.hs
:ARCHIVE_TIME: 2013-05-24 Fri 18:07
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: controller
:ARCHIVE_CATEGORY: hyperq
:END:

#+begin_src haskell
-- Example
--
-- $ ghc --make Controller.hs
-- $ ./Controller
import ControllerTest
import System.Environment
import Data.Maybe

main :: IO ()
main = do
     a <- getArgs
     let f = fromMaybe "../dot/candidate.dot" $ listToMaybe a 
     dotGraph <- importDotFile f 
     putStrLn "nodes:"
     putStrLn $ show $ nodeList dotGraph
     putStrLn "connections:"
     putStrLn $ show $ edgeList dotGraph
     return ()

#+end_src

#+results:
: <interactive>:18:19: Not in scope: `edgeList'
: 
: <interactive>:18:28: Not in scope: `dotGraph'

** dot files
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:40
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: system specification
:ARCHIVE_CATEGORY: hyperq
:END:

Some alternative graphs for testing purposes:

*** sandpit dot

#+begin_src dot :file dot/sandpit2.png :cmdline -Kdot -Tpng :exports both
digraph G {
        node [label="\N"];
        node [style=filled, color="#1f3950",fontcolor="#eeeeee",shape=box]; 
        subgraph cluster_market_data {
                graph [label="market data", color="#909090"];
                {rank=min; dataaggregator [shape=egg,color="#cc11cc22",fontcolor="#101010",label="market(s)"];}
                localport [label="local market data stream"];
                dataaggregator -> localport [dir=both];
        }
        subgraph cluster_offwire {
                graph [label="offwire",
                        color="#909090"];
                offwirealgo [label="offline algo"];
                observer;
                databases;
                observer -> databases [color=red,label="write",fontcolor=red];
        }
        subgraph cluster_onwire {
                graph [label="onwire",
                        color="#909090"];
                node [style=filled];
                disruptor [label="event server"];
                eventalgo [label="algo"];
                controller;
                controller -> eventalgo [color="#aaaaaa",dir=both]
                disruptor -> listener;
                disruptor -> eventalgo;
                disruptor -> controller;
                controller -> disruptor [color="#0080ff"];
        }
        subgraph cluster_broker {
                graph [label="broker",
                        color="#909090"];
                brokeraggregator [shape=egg,color="#cc11cc22",fontcolor="#101010",label="broker(s)"];
                brokeraggregator -> trader [dir=both];
        }
        localport -> observer [color="#aaaaaa",style=dotted];
        controller -> localport [color="#aaaaaa"];
        localport -> disruptor [color="#0080ff"];
        listener -> observer [color="#aaaaaa",style=dotted];
        controller -> observer [color="#aaaaaa",style=dotted];
        controller -> trader [color="#aaaaaa",dir=both];
        controller -> offwirealgo [color="#aaaaaa",dir=both];
        databases -> offwirealgo [color=red,label="read",fontcolor=red];
        trader -> observer [color="#aaaaaa",style=dotted];
        eventalgo -> observer [color="#aaaaaa",style=dotted];
        offwirealgo -> observer [color="#aaaaaa",style=dotted];
}
#+end_src

#+results:
[[file:dot/sandpit2.png]]

** TODO controller
:LOGBOOK:
CLOCK: [2013-03-23 Sat 17:00]--[2013-03-23 Sat 17:55] =>  0:55
:END:
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:41
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_CATEGORY: hyperq
:ARCHIVE_TODO: TODO
:END:
*** Priority tasks:

- [ ] relate to dot code in [[*candidate%20dot][candidate dot]]

  The idea is to /start/ with a dot graph and use this to register each
  component and the messaging between components.
  - [ ] register nodes from candidate dot
  - [ ] register edges (which will use STM or common messaging systems)

The Controller module is both a component of the overall system and is the complete system.

*** ControllerTest.hs
:PROPERTIES:
:tangle:   haskell/hyperqdev/ControllerTest.hs
:END:

#+begin_src haskell
  module ControllerTest 
  ( importDotFile
  , importDot
  , printGraph
  , nodeList
  , edgeList
  ) where

  import Data.GraphViz
  import Data.GraphViz.Attributes.Complete
  import Data.GraphViz.Types
  import qualified Data.Text.Lazy as L
  import qualified Data.Text.Lazy.IO as I
  -- import qualified Data.GraphViz.Types.Generalised as G
  import Data.Graph.Inductive.Graph
  import qualified Data.Map as Map
  import Data.Map(Map)

  importDotFile :: FilePath -> IO (DotGraph String)
  importDotFile f = do
          dotText <- I.readFile f 
          return $ parseDotGraph dotText

  importDot :: L.Text -> DotGraph Node
  importDot s = parseDotGraph s

  printGraph :: DotGraph String -> IO ()
  printGraph d = do
          putStrLn $ L.unpack $ printDotGraph d
          return()

  nodeList :: DotGraph String -> [String]
  nodeList g = map nodeID $ graphNodes g

  edgeList :: DotGraph String -> [(String,String)]
  edgeList g =  map (\x -> (fromNode x, toNode x)) $ graphEdges g

  data CompType = Internal 
                | External
                deriving Show

  comp :: DotGraph String -> [(String, CompType)]
  comp g = zip name s where
       name = map nodeID n
       s =  map compType $ map nodeAttributes n
       n = toDotNodes $ nodeInformationClean True g

  toDotNodes :: (Ord n) => NodeLookup n -> [DotNode n]
  toDotNodes k = map $ (\(n,(_,as)) -> DotNode n as) . Map.assocs k

  dir :: [Attribute] -> DirType
  dir [] = Forward
  dir x = 
      let d = [a | Dir a <- x]
      in case d of
         [] -> Forward
         _ -> head d

  compType :: [Attribute] -> CompType
  compType [] = Internal
  compType x = 
      let d = [a | Shape a <- x]
      in case d of
         [] -> Internal
         [Egg] -> External
         _ -> Internal


  comm :: DotGraph String -> [(String, String, DirType)]
  comm g = zip3 from to d where 
       e = edgeInformationClean True g
       d = map dir $ map edgeAttributes e
       from = map fromNode e
       to = map toNode e

  data CommType = Read 
                | Write
                deriving Show


  commChan :: (String, String, DirType) -> [(String, String, CommType)]
  commChan (f,t,Both)          = [(f,t,Write)
                                 ,(f,t,Read)]
  commChan (f,t,Forward)       = [(f,t,Write)]
  commChan (f,t,Back)          = [(f,t,Read)]
  commChan _                   = []
#+end_src

*** edges

#+begin_src haskell
  import ControllerTest
  g <- importDotFile "./dot/iqcontroller.dot"
  map commChan $ comm g 
#+end_src

#+results:
: *ControllerTest Data.GraphViz L I G Data.Graph.Inductive.Graph Data.GraphViz.Attributes.Complete ControllerTest> [[("controller","admin",Write),("controller","admin",Read)],[("admin","adminport",Write),("admin","adminport",Read)],[("controller","marketport",Write),("controller","marketport",Read)],[("controller","lookupport",Write),("controller","lookupport",Read)],[("controller","STDIN",Read)],[("controller","STDOUT",Write)]]

#+begin_src haskell
dir :: [Attribute] -> DirType
dir x = [a | Dir a <- x]
#+end_src

#+results:
: <interactive>:209:7: parse error on input `='


*** nodes

#+begin_src haskell
import ControllerTest
import Data.List
g <- importDotFile "../dot/test.unit2.dot"
map (\x -> [x]) $ nodeList g
#+end_src

#+results:
| aggregator       |
| broker           |
| brokeraggregator |
| controller       |
| databases        |
| disruptor        |
| eventalgo        |
| exchange         |
| listener         |
| localport        |
| observer         |
| offwirealgo      |
| trader           |



*** commandline

#+begin_src sh :results output
cd ~/projects/hyperq/haskell
ghc --make Controller.hs
./Controller
#+end_src

#+results:
: nodes:
: ["aggregator","broker","brokeraggregator","controller","databases","disruptor","eventalgo","exchange","listener","localport","observer","offwirealgo","trader"]
: connections:
: [("exchange","aggregator"),("aggregator","localport"),("observer","databases"),("controller","eventalgo"),("disruptor","listener"),("disruptor","eventalgo"),("disruptor","controller"),("controller","disruptor"),("broker","brokeraggregator"),("brokeraggregator","trader"),("localport","observer"),("controller","localport"),("localport","disruptor"),("listener","observer"),("controller","observer"),("controller","trader"),("controller","offwirealgo"),("databases","offwirealgo"),("trader","observer"),("eventalgo","observer"),("offwirealgo","observer")]

** Feed
:PROPERTIES:
:tangle:   haskell/hyperqdev/Feed.hs
:ARCHIVE_TIME: 2013-06-12 Wed 16:42
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed/haskell interfacing
:ARCHIVE_CATEGORY: hyperq
:END:

no automation or control yet
- all incoming data gets written to a file specified in args
- input via stdin

To compile and run:

#+begin_src sh :results output :tangle no
cd haskell/hyperqdev
ghc --make Feed.hs threaded
./Feed data.out
#+end_src

#+begin_src haskell
  import Control.Concurrent
  import Network
  import System.Environment
  import System.Process
  import System.IO
  import Control.Exception
  import System.Exit
  import Control.Monad (forever)
  import Data.Time.Clock
  import Data.Time.Format
  import Data.Time.Calendar
  import System.Locale
  
  
  con :: String -> String -> IO ()
  con host port = do
      h <- connectTo host $ PortNumber $ toEnum $ read port
      hSetBuffering stdout LineBuffering
      hSetBuffering h      LineBuffering
      done <- newEmptyMVar
  
      _ <- forkIO $ (hGetContents h >>= putStr)
                  `finally` tryPutMVar done ()
  
      _ <- forkIO $ (getContents >>= hPutStr h)
                  `finally` tryPutMVar done ()
  
                  -- Wait for at least one of the above threads to complete
      takeMVar done
  
  conFileTime :: String -> String -> String -> IO ()
  conFileTime host port file = do
      h <- connectTo host $ PortNumber $ toEnum $ read port
      f <- openFile file WriteMode
      hSetBuffering stdout LineBuffering
      hSetBuffering h      LineBuffering
      hSetBuffering f      LineBuffering
      done <- newEmptyMVar
  
      _ <- forkIO $ forever (do
                          t <- getCurrentTimeString
                          st <- hGetLine h
                          hPutStrLn f $ t ++ "," ++ st)
                  `finally` tryPutMVar done ()
  
      _ <- forkIO $ (getContents >>= hPutStr h)
                  `finally` tryPutMVar done ()
  
                  -- Wait for at least one of the above threads to complete
      takeMVar done
  
  conAdmin :: String -> IO ()
  conAdmin cmds = do
    con "localhost" "9300"
    putStr cmds
  
  conStream :: String -> IO ()
  conStream cmds = do
    con "localhost" "5009"
    putStr cmds
  
  conLookup :: String -> IO ()
  conLookup cmds = do
    con "localhost" "9100"
    putStr cmds
  
  logon :: IO ()
  logon = do
    let cmd = "wine"
        args = ["Z:\\Users\\tonyday\\wine\\iqfeed\\iqconnect.exe", "-product IQFEED_DEMO -version 1"]
    _ <- rawSystem cmd args
    return()
  
  
  getCurrentTimeString :: IO String
  getCurrentTimeString = do
     now <- getCurrentTime
     let offset = diffUTCTime  (UTCTime (ModifiedJulianDay 0) (secondsToDiffTime 0)) (UTCTime (ModifiedJulianDay 0) (secondsToDiffTime (4 * 60 * 60)))
     return (formatTime defaultTimeLocale "%H:%M:%S%Q" $ addUTCTime offset now)
  
  
  main :: IO ExitCode
  main = do
    [file] <- getArgs
    _ <- forkIO (logon)
    threadDelay $ 1000000 * 10
    putStr "\ndelay finished\n"
    conFileTime "localhost" "5009" file
    return(ExitSuccess)
#+end_src

** Hyperq.Iqconnect
:PROPERTIES:
:tangle:   haskell/Hyperq/Iqconnect.hs
:ARCHIVE_TIME: 2013-06-12 Wed 16:42
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed/haskell interfacing
:ARCHIVE_CATEGORY: hyperq
:END:

#+begin_src haskell :tangle haskell/Hyperq/Iqconnect.hs
module Hyperq.Iqconnect where 

import Control.Concurrent
import Network
import System.Process
import System.IO
import Control.Exception
import Control.Monad (forever)
import Data.Time.Clock
import Data.Time.Format
import Data.Time.Calendar
import System.Locale

con :: String -> String -> IO ()
con host port = do
    h <- connectTo host $ PortNumber $ toEnum $ read port
    hSetBuffering stdout LineBuffering
    hSetBuffering h      LineBuffering
    done <- newEmptyMVar

    _ <- forkIO $ (hGetContents h >>= putStr)
                `finally` tryPutMVar done ()

    _ <- forkIO $ (getContents >>= hPutStr h)
                `finally` tryPutMVar done ()

                -- Wait for at least one of the above threads to complete
    takeMVar done

conFileTime :: String -> String -> String -> IO ()
conFileTime host port file = do
    h <- connectTo host $ PortNumber $ toEnum $ read port
    f <- openFile file WriteMode
    hSetBuffering stdout LineBuffering
    hSetBuffering h      LineBuffering
    hSetBuffering f      LineBuffering
    done <- newEmptyMVar

    _ <- forkIO $ forever (do
                        t <- getCurrentTimeString
                        st <- hGetLine h
                        hPutStrLn f $ t ++ "," ++ st)
                `finally` tryPutMVar done ()

    _ <- forkIO $ (getContents >>= hPutStr h)
                `finally` tryPutMVar done ()

                -- Wait for at least one of the above threads to complete
    takeMVar done

conAdmin :: String -> IO ()
conAdmin cmds = do
  con "localhost" "9300"
  putStr cmds

conStream :: String -> IO ()
conStream cmds = do
  con "localhost" "5009"
  putStr cmds

conLookup :: String -> IO ()
conLookup cmds = do
  con "localhost" "9100"
  putStr cmds

logon :: IO ()
logon = do
  let cmd = "wine"
      args = ["Z:\\Users\\tonyday\\wine\\iqfeed\\iqconnect.exe", "-product IQFEED_DEMO -version 1"]
  _ <- rawSystem cmd args
  return()


getCurrentTimeString :: IO String
getCurrentTimeString = do
   now <- getCurrentTime
   let offset = diffUTCTime  (UTCTime (ModifiedJulianDay 0) (secondsToDiffTime 0)) (UTCTime (ModifiedJulianDay 0) (secondsToDiffTime (4 * 60 * 60)))
   return (formatTime defaultTimeLocale "%H:%M:%S%Q" $ addUTCTime offset now)
#+end_src

** Iqtest
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:42
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed/haskell interfacing
:ARCHIVE_CATEGORY: hyperq
:END:

#+begin_src haskell :tangle haskell/test/Iqtest.hs
import Control.Concurrent
import System.Environment
import System.Exit
import Hyperq.Iqconnect

main :: IO ExitCode
main = do
  [file] <- getArgs
  _ <- forkIO (logon)
  threadDelay $ 1000000 * 10
  putStr "\ndelay finished\n"
  conFileTime "localhost" "5009" file
  return(ExitSuccess)
#+end_src

** register
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:44
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed/haskell interfacing/Iqtest development
:ARCHIVE_CATEGORY: hyperq
:END:

*** channels

#+begin_src haskell
  import ControllerTest
  g <- importDotFile "./dot/iqcontroller.dot"
  map commChan $ comm g 
#+end_src

#+results:
| (controller admin Write)      | (controller admin Read)      |
| (admin adminport Write)       | (admin adminport Read)       |
| (controller marketport Write) | (controller marketport Read) |
| (controller lookupport Write) | (controller lookupport Read) |
| (controller stdin Read)       |                              |
| (controller stdout Write)     |                              |

*** components

#+begin_src haskell
  import ControllerTest
  g <- importDotFile "./dot/iqcontroller.dot"
  comp g 
#+end_src

#+results:
| admin      | Internal |
| adminport  | External |
| controller | Internal |
| lookupport | External |
| marketport | External |
| stdin      | Internal |
| stdout     | Internal |

** echo-test
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:44
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed/haskell interfacing/Iqtest development
:ARCHIVE_CATEGORY: hyperq
:END:
components
- cStdin 
    - reads from stdin
    - sequences to cController<-cStdin
- cController
    - consumes from cController<-cStdin
    - sequences to cStdout<-cController
- cStdout
    - consumes from cStdout<-cController
    - writes to stdout


*** dot
:PROPERTIES:
:tangle:   Test/echo-test.dot
:END:

#+begin_src dot :file img/echo-test.png :cmdline -Kdot -Tpng :exports both
  digraph G {
          node [label="\N"];
          node [style=filled, color="#1f3950",fontcolor="#eeeeee",shape=box];
          controller -> stdin [color="#aaaaaa", dir=back]
          controller -> stdout [color="#aaaaaa"]
  }
#+end_src

*** buffers

#+begin_src haskell
  import ControllerTest
  g <- importDotFile "./dot/echo-test.dot"
  buffs g
#+end_src

#+results:
: fromList [("controller",["stdout"]),("stdin",["controller"])]

** background workers
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:45
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed/haskell interfacing
:ARCHIVE_CATEGORY: hyperq
:END:

http://www.haskell.org/haskellwiki/Background_thread_example

http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/index.htm



- [X] understand stm operations
- [ ] create x components
    - a component has:
      - maybe an emitter (is this a writer?)
      - maybe a receiver (is this a reader)
      - maybe a fn(receiver) = emission
    - [ ] work out which has which from dot
    - [ ] fake producers (for relevant components)
- [ ] create readers and writers
    - [ ] writers (producers) need consumers?

** latency research
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:45
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed/haskell interfacing
:ARCHIVE_CATEGORY: hyperq
:END:

I collected trade and order ticks for 12 contracts on 14th March from iqfeed,
and timestamped each tick with current system time. There are two different
potential points at which to measure latency:
- iqfeed sends a ping every second, and
- each quote has a relevant market timestamp to the millisecond

*** feed ping latency
  
From the raw iqfeed heartbeat:

    #+begin_src R
      t = read.csv("data/streamt.txt",header=FALSE,as.is=TRUE)
      pingtime = strptime(t[,3], "%Y%m%d %H:%M:%S")
      stamp = strptime(paste(strftime(pingtime,"%Y%m%d"), t[,1], sep=" "), "%Y%m%d %H:%M:%OS")    
      latency = as.double(stamp - pingtime)
      df = data.frame(pingtime=pingtime, latency=latency)
      summary(df)
    #+end_src

    #+results:
    | Min.   :2013-03-14 07:30:57 | Min.   :-0.90665 |
    | 1st Qu.:2013-03-14 17:15:41 | 1st Qu.:-0.01492 |
    | Median :2013-03-15 03:02:15 | Median : 0.14950 |
    | Mean   :2013-03-15 03:01:28 | Mean   : 0.38876 |
    | 3rd Qu.:2013-03-15 12:46:33 | 3rd Qu.: 0.22824 |
    | Max.   :2013-03-15 22:33:24 | Max.   : 7.89887 |
    | NA's   :1                   | NA's   :1        |

    #+begin_src R
    require(ggplot2)
    qplot(data=df, x=pingtime, y=latency)
    ggsave("ping-latency.svg")
    #+end_src

    #+results:

    [[file:data/ping-latency.svg]]

    The simple scatterplot shows many negative values, especially when the
    market is open, and a step jump in the later pings (when no quotes were
    being recorded).  These jumps may be due to changes in my system clock
    (automatic appletime resolutions) or due to a lack of accuracy in the
    iqfeed pings.

    Scatterplots tend to provide dubious visualisation for bigdata, and a new
    package out that helps is [[http://vita.had.co.nz/papers/bigvis.html][bigvis]].

    bigvis is not yet available at CRAN but can be installed via a github
    repository (see https://github.com/hadley/bigvis for details).

    #+begin_src R
    install.packages("devtools")
    devtools::install_github("bigvis")
    #+end_src

    bigvis doesn't handle non-numeric data (like time), so rather than
    autopilot, I use ggplot directly.   

    #+begin_src R :results file
      require(bigvis)
      require(ggplot2)
      dfn = condense(bin(as.double(df$pingtime),60),bin(df$latency,.1))
      dfg = data.frame(as.POSIXct(dfn[,1],origin="1960-01-01", tz="GMT"),dfn[,2],dfn[,3])
      colnames(dfg) = c("Time","Latency","Count")
      g = ggplot(data=dfg,aes(x=Time,y=Latency))
      g + geom_tile(aes(fill=Count)) + scale_fill_gradient(low="#e5e5e5", high = "#444548") + scale_y_continuous(limits=c(-1,1))
      ggsave("img/ping-latency-condensed.png")
   #+end_src

   [[file:img/ping-latency-condensed.png]]

   Using the bigvis techniques clarifies a few main issues for further research:
   - there is a step jump near market open where the majority of the pings
     jump from around 250 msecs to -750 msecs. This looks like either a coding
     error or the ping being off by up to a second.
   - during market open (when tick volume is high) ping can vary by a second.
   


*** disconnects
   Just looking at the ping counts after binning into one minute intervals:
   
   #+begin_src R
      df.dis = condense(bin(as.double(df$pingtime),60))
      dfg = data.frame(as.POSIXct(df.dis[,1],origin="1960-01-01", tz="GMT"),60-df.dis[,2])
      colnames(dfg) = c("Time","Count")
      g = ggplot(data=dfg,aes(x=Time,y=Count))
      g + geom_line(aes())
      ggsave("img/disconnects.png")

   #+end_src

   [[file:img/disconnects.png]]

   iqfeed regularly suffers from disconnects with reconnection occuring within
   a minute.


*** event latency

from the R database of the one day quote ticks...

- open data
  #+begin_src R
  
  rm(list = ls())
  require("mmap")
  require("rindex")
  require("plyr")
  require("stringr")
  raw.stream = "streamqh"
  # where the mmap db is located
  db.path = paste("data/",raw.stream,"/",sep="")
  
  load(paste(db.path,".Rdbinfo",sep=""))
  #m = mmap(main.filename, mode=st)
  stream = NULL
  stream$stamp = mmap(paste(db.path,fields[1],".data",sep=""), mode=double())
  stream$code = mmap(paste(db.path,fields[2],".data",sep=""), mode=char(1))
  stream$symbol = mmap(paste(db.path,fields[3],".data",sep=""), mode=char(ticker.length))
  stream$trade = mmap(paste(db.path,fields[4],".data",sep=""), mode=double())
  stream$vol = mmap(paste(db.path,fields[5],".data",sep=""), mode=integer())
  stream$tradetime = mmap(paste(db.path,fields[6],".data",sep=""), mode=double())
  stream$tradeex = mmap(paste(db.path,fields[7],".data",sep=""), mode=double())
  stream$volex = mmap(paste(db.path,fields[8],".data",sep=""), mode=integer())
  stream$tradetimeex = mmap(paste(db.path,fields[9],".data",sep=""), mode=double())
  stream$voltot = mmap(paste(db.path,fields[10],".data",sep=""), mode=integer())
  stream$bid = mmap(paste(db.path,fields[11],".data",sep=""), mode=double())
  stream$bidvol = mmap(paste(db.path,fields[12],".data",sep=""), mode=integer())
  stream$bidtime = mmap(paste(db.path,fields[13],".data",sep=""), mode=double())
  stream$ask = mmap(paste(db.path,fields[14],".data",sep=""), mode=double())
  stream$askvol = mmap(paste(db.path,fields[15],".data",sep=""), mode=integer())
  stream$asktime = mmap(paste(db.path,fields[16],".data",sep=""), mode=double())
  stream$event = mmap(paste(db.path,fields[17],".data",sep=""), mode=char(12))
  stream$id = mmap(paste(db.path,fields[18],".data",sep=""), mode=integer())
  
  #+end_src

  #+results:


- Define events and extract relevant times
  #+begin_src R
  n = length(stream$event[])
  
  tC = grepl("C",stream$event[])
  tO = grepl("O",stream$event[])
  ta = grepl("a",stream$event[])
  tb = grepl("b",stream$event[])
  ta = ta & !(tC | tO)
  tb = tb & !(tC | tO | ta)
  tother = !(ta | tb | tC | tO)
  
  event.category = (1 * tC) + (2 * tO) + (3 * ta) + (4 * tb) + (5 * tother)
  
  event.time = (stream$tradetime[] * tC +
          stream$tradetimeex[] * tO +
          stream$asktime[] * ta +
          stream$bidtime[] * tb +
          stream$tradetime[] * tother)
  
  event.time.posix = as.POSIXct(event.time,origin="1960-01-01", tz="GMT")
  event.stamp = stream$stamp[]
  
  event.latency = event.stamp - event.time  
  
  event.df = data.frame(symbol=stream$symbol[],event.category,event.time, event.stamp, event.latency)
  summary(event.df)
  #+end_src

  #+results:
  | @ESM13 :2553308 | Min.   :1.000 | Min.   :1.366e+09 | Min.   :1.366e+09 | Min.   :-85800.76 |
  | @NQM13 :1285545 | 1st Qu.:3.000 | 1st Qu.:1.366e+09 | 1st Qu.:1.366e+09 | 1st Qu.:     0.22 |
  | @YMM13 :1216006 | Median :3.000 | Median :1.366e+09 | Median :1.366e+09 | Median :     0.33 |
  | EBK13  : 917275 | Mean   :3.107 | Mean   :1.366e+09 | Mean   :1.366e+09 | Mean   :   226.44 |
  | @JYM13 : 844995 | 3rd Qu.:4.000 | 3rd Qu.:1.366e+09 | 3rd Qu.:1.366e+09 | 3rd Qu.:   600.22 |
  | EBM13  : 610827 | Max.   :5.000 | Max.   :1.366e+09 | Max.   :1.366e+09 | Max.   :  9818.25 |
  | (Other):1373320 | nil           | nil               | nil               | nil               |

- bigvis manipulations
  #+begin_src R
  require("bigvis")
  require("ggplot2")
  df1 = condense(bin(event.df$event.time,60),bin(event.df$event.latency,0.05))
  df2 = df1[(df1$event.df.event.latency > 0) & (df1$event.df.event.latency < 1),]   
  dfg = data.frame(as.POSIXct(df2[,1]+10*60*60,origin="1960-01-01", tz=""),df2[,2],df2[,3])
  colnames(dfg) = c("Time","Latency","Count")
  g = ggplot(data=dfg,aes(x=Time,y=Latency))
  g + geom_tile(aes(fill=Count)) + scale_fill_gradient(low="#e5e5e5", high = "#444548") + scale_y_continuous(limits=c(-1,1))
  ggsave("img/quote-latency-condensed.svg")

  #+end_src

  #+results:

  [[file:img/quote-latency-condensed.svg]]

  Unlike the iqfeed ping, there is a consistent latency pattern when comparing
  market stamp and local system stamp, with no spurious negative values.

- symbols

  #+begin_src R :results output
  summary(as.factor(stream$symbol[]))
  #+end_src

  #+results:
  : +SK13   +SPH13  @EDM13  @EDU13  @ESH13  @ESM13  @F1M13  @JYM13  @N1M13  @NQM13  
  :  299398     108  120731  167649  273192 2553308   27715  844995   27357 1285545 
  : @T1M13  @USNM13 @VMJ13  @YMM13  CRDJ13  EBK13   EBM13   
  :    1524   54804    3146 1216006  397696  917275  610827

- emini latency
  #+begin_src R
    ind.emini = indexEQ(ind.symbol,"@ESM13 ")
    df1 = condense(bin(event.df$event.time[ind.emini],600),bin(event.df$event.latency[ind.emini],0.05))
    df2 = df1[(df1$event.df.event.latency > -1) & (df1$event.df.event.latency < 10),]   
    dfg = data.frame(as.POSIXct(df2[,1]+10*60*60,origin="1960-01-01", tz=""),df2[,2],df2[,3])
    colnames(dfg) = c("Time","Latency","Count")
    g = ggplot(data=dfg,aes(x=Time,y=Latency))
    g + geom_tile(aes(fill=Count)) + scale_fill_gradient(low="#e5e5e5", high = "#444548") + scale_y_continuous(limits=c(-1,1))
    ggsave("img/quote-latency-condensed-emini.svg")
  #+end_src

- average latency (with binning)
  #+begin_src R
    require(ggplot2)
    require(bigvis)
    ind.emini = indexEQ(ind.symbol,"@ESM13 ")
    df1 = condense(bin(event.df$event.time[ind.emini],300,name="time"),bin(event.df$event.latency[ind.emini],0.05,name="latency"))
    df2 = df1[(df1$latency > 0) & (df1$latency < 2),]
    lat.av = tapply(df2$latency*df2$.count,df2$time,sum)/tapply(df2$.count,df2$time,sum)
    dfg = data.frame(Time=as.POSIXct(as.double(row.names(lat.av))+10*60*60,origin="1960-01-01", tz=""),Latency=lat.av)
    #colnames(dfg) = c("Time","Latency","Count")
    g = ggplot(data=dfg,aes(x=Time,y=Latency))
    g + geom_point()
    ggsave("img/quote-latency-averagecondensed.svg")
  #+end_src

** R interfacing
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:45
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed
:ARCHIVE_CATEGORY: hyperq
:END:

Using R to read the raw feed proceeds along the following lines:

#+begin_src R
msg3<-"function=subscribe|item=MI.EQCON.1|schema=last_price;ask;bid" msg4<-"function=unsubscribe" 
#open socket connection 

socketPointer<-socketConnection('localhost', port=5333, server=FALSE) 
#subscribe 

writeLines(msg3, socketPointer) 
#read data from file 
readLines(con=socketPointer,n=1,ok=TRUE,warn=TRUE,encoding='UTF-8') 
#unsubscribe 

writeLines(msg4, socketPointer) 
#close socket 

close(socketPointer)

#+end_src


#+begin_src R :session *Rlogon* :results output
  rm(list = ls())
  code.startup = system2("wine", "\"Z:\\\\Users\\\\tonyday\\\\wine\\\\iqfeed\\\\iqconnect.exe\"", stdout="", stderr="",wait=FALSE)
  Sys.sleep(10)
  socketAdmin=socketConnection('localhost', port=9300, open="a+") 
  Sys.sleep(1)
  if (isOpen(socketAdmin)) {
    response.initial.stream = readLines(socketAdmin)
    print(response.initial.stream)
  } else {
    print("login failed")
  }
#+end_src

#+results:
#+begin_example
Wine cannot find the FreeType font library.  To enable Wine to
use TrueType fonts please install a version of FreeType greater than
or equal to 2.0.5.
http://www.freetype.org
Wine cannot find the FreeType font library.  To enable Wine to
use TrueType fonts please install a version of FreeType greater than
or equal to 2.0.5.
http://www.freetype.org
Wine cannot find the FreeType font library.  To enable Wine to
use TrueType fonts please install a version of FreeType greater than
or equal to 2.0.5.
http://www.freetype.org
Wine cannot find the FreeType font library.  To enable Wine to
use TrueType fonts please install a version of FreeType greater than
or equal to 2.0.5.
http://www.freetype.org
Wine cannot find the FreeType font library.  To enable Wine to
use TrueType fonts please install a version of FreeType greater than
or equal to 2.0.5.
http://www.freetype.org
fixme:heap:HeapSetInformation 0x0 1 0x0 0
[1] "S,STATS,66.112.156.222,60003,500,0,1,0,0,0,Mar 12 5:53AM,Mar 12 5:53AM,Connected,5.0.0.9,414096,0.17,0.02,0.03,0.00,0.0,0.00,"
#+end_example



R sucks at asynchronous programming.

** April23
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:46
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed
:ARCHIVE_CATEGORY: hyperq
:END:

Hacking around with the April 23 data
- [ ] create module from Feed code
  [[http://www.haskell.org/tutorial/modules.html][A Gentle Introduction to Haskell: Modules]]

- [ ] level 1 historical tick data download
- [ ] level 2 ticks
- [ ] timestamp for first twitter post

** haskell
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:46
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: event stream engineering/messaging
:ARCHIVE_CATEGORY: hyperq
:END:
http://community.haskell.org/~simonmar/par-tutorial.pdf

** time
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:47
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: clipboard
:ARCHIVE_CATEGORY: hyperq
:END:

http://apple.stackexchange.com/questions/82878/how-to-tell-when-time-has-been-synced-with-ntp-server-in-mac-os-x-lion

** event stream engineering
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-12 Wed 16:49
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_CATEGORY: hyperq
:END:

** travis repo insertion
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-13 Thu 08:33
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_CATEGORY: hyperq
:END:
     data-ringbuffer will require separate build as follows:

     git clone http://github.com/simonmichael/hledger.git
     cd hledger
     cabal update
     cabal install ./hledger-lib ./hledger [./hledger-web]

     http://hledger.org/INSTALL.html

** sornette
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-13 Thu 08:35
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: references/algo
:ARCHIVE_CATEGORY: hyperq
:END:

http://arxiv.org/find/all/1/all:+sornette/0/1/0/all/0/1

some specific articles

http://arxiv.org/pdf/cond-mat/0301543.pdf
http://arxiv.org/pdf/1108.0077.pdf
http://arxiv.org/ftp/arxiv/papers/1012/1012.4118.pdf
http://arxiv.org/pdf/1011.2882.pdf
http://arxiv.org/pdf/1007.2420.pdf
http://arxiv.org/pdf/0909.1007.pdf
http://arxiv.org/ftp/arxiv/papers/0812/0812.2449.pdf
http://arxiv.org/pdf/0909.1007.pdf

** existing design choice
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-13 Thu 08:41
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: project design
:ARCHIVE_CATEGORY: hyperq
:END:

The typical system platform choices for the aspiring fast trader can be boiled down
to a few options:

- pure HFT
  
  Proprietary real-time event processing that almost has to be in a language with no garbage
  collection (C, C++ or Java) to avoid millisec delays (ie low
  latency processes).
  - dedicated co-located servers
  - real-time order book analytics
  - real-time price information streams 
  - algorithms 'hard-wired'

- Big Data 

  A statistical R&D undertaking using matlab, R, julia and fast database
  technology.

- Old school
  
  Modification of a human-centric trading system focused on broker interfaces
  and visualization technologies (charting).

Whatever the orientation there are significant weaknesses to existing
solutions:
- most open source projects come with strings attached and are largely
  inducements to purchase this or that commercial service or product.
- build-your-own solutions need to be largely built from scratch.

** system specification
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-13 Thu 08:42
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_CATEGORY: hyperq
:END:

hyperq is in an experimental phase and, as such, there is a need for flexibility
in the top-down design of the system. To achieve this, the overall design is
first being modelled using graphviz.  The current candidate system is:

** other choices
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-13 Thu 08:43
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed/iqfeed
:ARCHIVE_CATEGORY: hyperq
:END:
[[http://www.strategyquant.com/tickdatadownloader/][Tick Data Downloader]]
[[http://www.kinetick.com/features][Kinetick - Streaming real time quotes and historical market data - features]]

** NEXT haskell interfacing
:PROPERTIES:
:ARCHIVE_TIME: 2013-06-13 Thu 08:45
:ARCHIVE_FILE: ~/projects/hyperq/hyperq.org
:ARCHIVE_OLPATH: market data feed
:ARCHIVE_CATEGORY: hyperq
:ARCHIVE_TODO: NEXT
:END:
*** NEXT Iqtest development
SCHEDULED: <2013-06-03 Mon>
:LOGBOOK:
CLOCK: [2013-03-24 Sun 17:15]--[2013-03-26 Tue 11:26] => 42:11
:END:
:PROPERTIES:
:tangle:   haskell/hyperqdev/Iqtest.hs
:END:

